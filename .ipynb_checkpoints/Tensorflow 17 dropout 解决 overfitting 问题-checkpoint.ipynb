{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "#load data\n",
    "digits=load_digits()\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "y=LabelBinarizer().fit_transform(y)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3)\n",
    "\n",
    "def add_layer(inputs,in_size,out_size,layer_name,activation_function=None):\n",
    "    #add one more layer and return the output of this layer\n",
    "    Weights=tf.Variable(tf.random_normal([in_size,out_size]))\n",
    "    biases=tf.Variable(tf.zeros([1,out_size])+0.1)\n",
    "    Wx_plus_b=tf.matmul(inputs,Weights)+biases\n",
    "    Wx_plus_b=tf.nn.dropout(Wx_plus_b,keep_prob)\n",
    "    if activation_function is None:\n",
    "        outputs=Wx_plus_b\n",
    "    else:\n",
    "        outputs=activation_function(Wx_plus_b)\n",
    "    tf.summary.histogram(layer_name+'/outputs',outputs)\n",
    "    return outputs\n",
    "\n",
    "#define placeholder for inputs to network\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "xs=tf.placeholder(tf.float32,[None,64]) #8*8\n",
    "ys=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#add hidden layer\n",
    "l1=add_layer(xs,64,50,'l1',activation_function=tf.nn.tanh)\n",
    "#add output layer\n",
    "prediction=add_layer(l1,50,10,'l2',activation_function=tf.nn.softmax)\n",
    "\n",
    "#the loss between prediction and real data\n",
    "cross_entropy=tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[1])) #loss\n",
    "tf.summary.scalar('loss',cross_entropy)\n",
    "\n",
    "train_step=tf.train.GradientDescentOptimizer(0.6).minimize(cross_entropy)\n",
    "\n",
    "sess=tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "#summary writer goes in here\n",
    "train_writer=tf.summary.FileWriter(\"logs/train\",sess.graph)\n",
    "test_writer=tf.summary.FileWriter(\"logs/test\",sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(500):\n",
    "    sess.run(train_step,feed_dict={xs:X_train,ys:y_train,keep_prob:0.5})\n",
    "    if i%50==0:\n",
    "        #record loss\n",
    "        train_result=sess.run(merged,feed_dict={xs:X_train,ys:y_train,keep_prob:1})\n",
    "        test_result=sess.run(merged,feed_dict={xs:X_test,ys:y_test,keep_prob:1})\n",
    "        train_writer.add_summary(train_result,i)\n",
    "        test_writer.add_summary(test_result,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
